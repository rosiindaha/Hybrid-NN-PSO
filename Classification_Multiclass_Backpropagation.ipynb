{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fungsi_save.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pilih data fitur masukan NN1\n",
    "train_data = pd.read_csv('data_training.csv', sep=',',header=None)\n",
    "train_data = train_data.values\n",
    "post = pd.read_csv('data_featureclass_pos.csv', sep=',',header=None)\n",
    "post = np.int_(post)\n",
    "total_features = np.size(train_data, axis=1)-2\n",
    "print(total_features)\n",
    "output_train_data = train_data[:,total_features] #output aja lym\n",
    "output_train_data = np.int_(output_train_data)\n",
    "train_data = train_data[:,:total_features]\n",
    "output_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [] #index fitur yang gak kepake\n",
    "for i in range(len(post)):\n",
    "    if post[i,0] == 0:\n",
    "        a.append(i)\n",
    "train_data = np.delete(train_data, a ,axis=1) #data fitur yang udah terseleksi\n",
    "print(\"Number of training samples is\", len(train_data))\n",
    "print(np.size(train_data,axis=1))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data validasi\n",
    "validasi_data = pd.read_csv('data_testing.csv', sep=',',header=None)\n",
    "validasi_data = validasi_data.values\n",
    "output_validasi_data = validasi_data[:,total_features] #output aja untuk output 1\n",
    "output_validasi_data = np.int_(output_validasi_data)\n",
    "validasi_data = validasi_data[:,0:total_features] #fitur aja tanpa output dan fiturnya lengkap\n",
    "validasi_data = np.delete(validasi_data, a ,axis=1) #data fitur yang udah terseleksi\n",
    "\n",
    "#save_data1(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = np.size(validasi_data,1)\n",
    "n_hidden = 20\n",
    "n_classes = 3\n",
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(output_train_data)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(onehot_encoded)#output_train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/np.amax(train_data, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error1 = []\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "activation_function = sigmoid\n",
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "class NeuralNetwork:\n",
    "        \n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 learning_rate,\n",
    "                 bias=None\n",
    "                ):  \n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "            \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices()\n",
    "    \n",
    "        \n",
    "    \n",
    "    def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural \n",
    "        network with optional bias nodes\"\"\"\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "        \n",
    "        rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n",
    "                                       self.no_of_in_nodes + bias_node))\n",
    "        rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n",
    "                                        self.no_of_hidden_nodes + bias_node))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, input_vector, target_vector):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        global error1\n",
    "        bias_node = 1 if self.bias else 0\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the inpuy_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "                                    \n",
    "            \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "        \n",
    "        output_vector1 = np.dot(self.weights_in_hidden, input_vector)\n",
    "        output_vector_hidden = activation_function(output_vector1)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        \n",
    "        output_vector2 = np.dot(self.weights_hidden_out, output_vector_hidden)\n",
    "        output_vector_network = activation_function(output_vector2)\n",
    "        \n",
    "        output_errors = target_vector - output_vector_network\n",
    "        error1.append(output_errors)\n",
    "        self.output_errors = output_errors\n",
    "        # update the weights:\n",
    "        tmp = output_errors * output_vector_network * (1.0 - output_vector_network)     \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n",
    "        self.weights_hidden_out += tmp\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.weights_hidden_out.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:]     # ???? last element cut off, ???\n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.weights_in_hidden += self.learning_rate * x\n",
    "    def error(self):\n",
    "        return self.output_errors\n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the inpuy_vector\n",
    "            input_vector = np.concatenate( (input_vector, [1]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        output_vector = np.dot(self.weights_in_hidden, input_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[1]]) )\n",
    "            \n",
    "        output_vector = np.dot(self.weights_hidden_out, output_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        output_vector2 = np.argmax(output_vector, axis=0)\n",
    "        W1 = self.weights_in_hidden\n",
    "        W2 = self.weights_hidden_out\n",
    "        save(W1,W2)\n",
    "        return output_vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_network = NeuralNetwork(no_of_in_nodes=n_inputs, \n",
    "                               no_of_out_nodes=3, \n",
    "                               no_of_hidden_nodes=n_hidden,\n",
    "                               learning_rate=0.1,\n",
    "                               bias=None)\n",
    "output_predict_train = []\n",
    "error = []\n",
    "for z in range(300):\n",
    "    for i in range(len(train_data)):\n",
    "        simple_network.train(train_data[i], labels[i])\n",
    "    print(simple_network.error())\n",
    "    error.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    print(\"data ke-\",i)\n",
    "    print(output_train_data[i])\n",
    "    print(simple_network.run(train_data[i]))\n",
    "    output_predict_train.append(simple_network.run(train_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predict_train = np.array(output_predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validasi_data = validasi_data/np.amax(validasi_data, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predict_validasi = []\n",
    "for i in range(len(validasi_data)):\n",
    "    print(i)\n",
    "    print(output_validasi_data[i])\n",
    "    print(simple_network.run(validasi_data[i]))\n",
    "    output_predict_validasi.append(simple_network.run(validasi_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predict_validasi = np.array(output_predict_validasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train = confusion_matrix(output_train_data, output_predict_train)\n",
    "cm_validasi = confusion_matrix(output_validasi_data, output_predict_validasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Presisi data training limfoblas : \", precision(1,cm_train))\n",
    "print(\"Recall data training limfoblas (sensitivitas) : \", recall(1,cm_train))\n",
    "print(\"Presisi rerata data training : \", precision_macro_average(cm_train))\n",
    "print(\"Presisi rerata data training : \", recall_macro_average(cm_train))\n",
    "print(\"Akurasi data training : \", accuracy(cm_train))\n",
    "print(\"F1 Score data training : \", f1_score(precision_macro_average(cm_train),recall_macro_average(cm_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Presisi data validasi limfoblas : \", precision(1,cm_validasi))\n",
    "print(\"Recall data validasi limfoblas\", recall(1,cm_validasi))\n",
    "print(\"Presisi rerata data validasi : \", precision_macro_average(cm_validasi))\n",
    "print(\"Presisi rerata data validasi : \", recall_macro_average(cm_validasi))\n",
    "print(\"Akurasi data validasi : \", accuracy(cm_validasi))\n",
    "print(\"F1 Score data validasi : \", f1_score(precision_macro_average(cm_validasi),recall_macro_average(cm_validasi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error1 = np.array(error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =np.array([i for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = 0\n",
    "cost_er = []\n",
    "for i in range(300):\n",
    "    cost = 0\n",
    "    for z in range(300):\n",
    "        cost = cost+abs(error1[z+(len(train_data)*i)])\n",
    "    cost_er.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_er = np.array(cost_er)\n",
    "cost_er.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_error = []\n",
    "for i in range(300):\n",
    "    cost_error.append((np.mean(cost_er[i]/300,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_error = np.array(cost_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch,cost_error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_t.figure.savefig('fix2(balanced)/bp/train4.png')\n",
    "cm_v.figure.savefig('fix2(balanced)/bp/validasi4.png')\n",
    "save_pos_step1('fix2(balanced)/bp/perfo_bp.csv',(accuracy(cm_train),accuracy(cm_validasi),cost_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
